# Milvus StorageV2 数据读写流程分析

## 1. 概述

StorageV2 的数据读写流程基于 Apache Arrow 格式，通过 PackedReader 和 PackedWriter 实现高效的批量数据操作。整个流程涉及数据序列化、Arrow 格式转换、存储后端交互等多个层次。

## 2. 数据写入流程

### 2.1 写入架构图

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   SyncPack      │    │  BulkPackWriter │    │  PackedWriter   │
│   (业务数据)     │───▶│   V2 (协调器)   │───▶│   (底层写入)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │
         ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   Insert Data   │    │  Arrow Record   │    │  Storage Files  │
│   Delete Data   │    │   Conversion    │    │   (分片存储)    │
│   Stats Data    │    │                 │    │                 │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### 2.2 数据同步任务 (SyncTask)

#### 2.2.1 任务结构定义
```go
type SyncTask struct {
    chunkManager        storage.ChunkManager      // 存储管理器
    allocator          allocator.Interface       // ID 分配器
    
    // 元数据信息
    collectionID       int64
    partitionID        int64
    segmentID          int64
    channelName        string
    
    // 数据包
    pack               *SyncPack
    
    // 结果存储
    insertBinlogs      map[int64]*datapb.FieldBinlog
    statsBinlogs       map[int64]*datapb.FieldBinlog
    deltaBinlog        *datapb.FieldBinlog
    
    // StorageV2 配置
    syncBufferSize      int64
    multiPartUploadSize int64
    storageConfig       *indexpb.StorageConfig
}
```

#### 2.2.2 写入流程控制
```go
func (t *SyncTask) Run(ctx context.Context) error {
    // 1. 获取 segment 信息
    segmentInfo := t.metacache.GetSegmentsBy(...)
    
    // 2. 根据存储版本选择写入器
    switch segmentInfo.GetStorageVersion() {
    case storage.StorageV2:
        // 使用 StorageV2 写入器
        writer := NewBulkPackWriterV2(
            t.metacache, 
            t.schema, 
            t.chunkManager, 
            t.allocator,
            t.syncBufferSize,        // 缓冲区大小
            t.multiPartUploadSize,   // 分片上传大小
            t.storageConfig,         // 存储配置
            t.writeRetryOpts...,     // 重试选项
        )
        
        // 执行写入
        t.insertBinlogs, t.deltaBinlog, t.statsBinlogs, 
        t.bm25Binlogs, t.flushedSize, err = writer.Write(ctx, t.pack)
        
    default:
        // 使用传统写入器
        writer := NewBulkPackWriter(...)
    }
    
    // 3. 更新元数据
    return t.writeMeta(ctx)
}
```

### 2.3 BulkPackWriterV2 实现

#### 2.3.1 写入器结构
```go
type BulkPackWriterV2 struct {
    *BulkPackWriter                    // 继承基础写入器
    schema              *schemapb.CollectionSchema
    bufferSize          int64          // 缓冲区大小
    multiPartUploadSize int64          // 分片上传大小
    storageConfig       *indexpb.StorageConfig
}
```

#### 2.3.2 核心写入方法
```go
func (bw *BulkPackWriterV2) Write(ctx context.Context, pack *SyncPack) (
    inserts map[int64]*datapb.FieldBinlog,
    deltas *datapb.FieldBinlog,
    stats map[int64]*datapb.FieldBinlog,
    bm25Stats map[int64]*datapb.FieldBinlog,
    size int64,
    err error,
) {
    // 1. 预分配 ID
    err = bw.prefetchIDs(pack)
    if err != nil {
        return
    }

    // 2. 写入插入数据
    if inserts, err = bw.writeInserts(ctx, pack); err != nil {
        return
    }
    
    // 3. 写入统计数据
    if stats, err = bw.writeStats(ctx, pack); err != nil {
        return
    }
    
    // 4. 写入删除数据
    if deltas, err = bw.writeDelta(ctx, pack); err != nil {
        return
    }
    
    // 5. 写入 BM25 统计
    if bm25Stats, err = bw.writeBM25Stats(ctx, pack); err != nil {
        return
    }

    size = bw.sizeWritten
    return
}
```

### 2.4 插入数据写入详细流程

#### 2.4.1 数据序列化
```go
func (bw *BulkPackWriterV2) writeInserts(ctx context.Context, pack *SyncPack) (
    map[int64]*datapb.FieldBinlog, error) {
    
    if len(pack.insertData) == 0 {
        return make(map[int64]*datapb.FieldBinlog), nil
    }
    
    // 1. 列分组策略
    columnGroups := storagecommon.SplitBySchema(bw.schema.GetFields())
    
    // 2. 序列化为 Arrow Record
    rec, err := bw.serializeBinlog(ctx, pack)
    if err != nil {
        return nil, err
    }
    
    // 3. 生成文件路径
    paths := make([]string, 0)
    for _, columnGroup := range columnGroups {
        path := metautil.BuildInsertLogPath(
            bw.getRootPath(), 
            pack.collectionID, 
            pack.partitionID, 
            pack.segmentID, 
            columnGroup.GroupID, 
            bw.nextID(),
        )
        paths = append(paths, path)
    }
    
    // 4. 创建 PackedWriter
    bucketName := paramtable.Get().ServiceParam.MinioCfg.BucketName.GetValue()
    w, err := storage.NewPackedRecordWriter(
        bucketName, 
        paths, 
        bw.schema, 
        bw.bufferSize, 
        bw.multiPartUploadSize, 
        columnGroups, 
        bw.storageConfig,
    )
    
    // 5. 写入数据
    if err = w.Write(rec); err != nil {
        return nil, err
    }
    
    // 6. 生成 Binlog 元数据
    logs := make(map[int64]*datapb.FieldBinlog)
    for _, columnGroup := range columnGroups {
        logs[columnGroup.GroupID] = &datapb.FieldBinlog{
            FieldID: columnGroup.GroupID,
            Binlogs: []*datapb.Binlog{
                {
                    LogSize:       int64(w.GetColumnGroupWrittenUncompressed(columnGroup.GroupID)),
                    LogPath:       w.GetWrittenPaths(columnGroup.GroupID),
                    EntriesNum:    w.GetWrittenRowNum(),
                    TimestampFrom: tsFrom,
                    TimestampTo:   tsTo,
                },
            },
        }
    }
    
    return logs, w.Close()
}
```

#### 2.4.2 Arrow Record 构建
```go
func (bw *BulkPackWriterV2) serializeBinlog(ctx context.Context, pack *SyncPack) (
    storage.Record, error) {
    
    // 1. 转换为 Arrow Schema
    arrowSchema, err := storage.ConvertToArrowSchema(bw.schema)
    if err != nil {
        return nil, err
    }
    
    // 2. 创建 Arrow Record Builder
    builder := array.NewRecordBuilder(memory.DefaultAllocator, arrowSchema)
    defer builder.Release()

    // 3. 构建 Record
    for _, chunk := range pack.insertData {
        if err := storage.BuildRecord(builder, chunk, bw.schema); err != nil {
            return nil, err
        }
    }

    // 4. 生成最终 Record
    rec := builder.NewRecord()
    field2Col := make(map[storage.FieldID]int, len(bw.schema.GetFields()))
    for c, field := range bw.schema.GetFields() {
        field2Col[field.FieldID] = c
    }
    
    return storage.NewSimpleArrowRecord(rec, field2Col), nil
}
```

### 2.5 PackedWriter 底层实现

#### 2.5.1 Writer 创建
```go
func NewPackedRecordWriter(
    bucketName string, 
    paths []string,                    // 文件路径列表
    schema *schemapb.CollectionSchema, // Collection Schema
    bufferSize int64,                  // 缓冲区大小
    multiPartUploadSize int64,         // 分片上传大小
    columnGroups []ColumnGroup,        // 列分组
    storageConfig *indexpb.StorageConfig, // 存储配置
) (*packedRecordWriter, error) {
    
    // 1. 转换为 Arrow Schema
    arrowSchema, err := ConvertToArrowSchema(schema)
    if err != nil {
        return nil, err
    }
    
    // 2. 处理文件路径
    storageType := storageConfig.GetStorageType()
    truePaths := lo.Map(paths, func(p string, _ int) string {
        if storageType == "local" {
            return p
        }
        return path.Join(bucketName, p)
    })
    
    // 3. 创建底层 PackedWriter
    writer, err := packed.NewPackedWriter(
        truePaths, 
        arrowSchema, 
        bufferSize, 
        multiPartUploadSize, 
        columnGroups, 
        storageConfig,
    )
    
    // 4. 初始化元数据
    columnGroupUncompressed := make(map[typeutil.UniqueID]uint64)
    pathsMap := make(map[typeutil.UniqueID]string)
    for i, columnGroup := range columnGroups {
        columnGroupUncompressed[columnGroup.GroupID] = 0
        pathsMap[columnGroup.GroupID] = paths[i]
    }
    
    return &packedRecordWriter{
        writer:                  writer,
        schema:                  schema,
        arrowSchema:             arrowSchema,
        bufferSize:              bufferSize,
        bucketName:              bucketName,
        pathsMap:                pathsMap,
        columnGroups:            columnGroups,
        columnGroupUncompressed: columnGroupUncompressed,
        storageConfig:           storageConfig,
    }, nil
}
```

#### 2.5.2 数据写入
```go
func (pw *packedRecordWriter) Write(r Record) error {
    var rec arrow.Record
    
    // 1. 转换为 Arrow Record
    if sar, ok := r.(*simpleArrowRecord); ok {
        rec = sar.r
    } else {
        // 手动构建 Arrow Record
        arrays := make([]arrow.Array, len(pw.schema.Fields))
        for i, field := range pw.schema.Fields {
            arrays[i] = r.Column(field.FieldID)
        }
        rec = array.NewRecord(pw.arrowSchema, arrays, int64(r.Len()))
    }
    
    // 2. 统计未压缩大小
    pw.rowNum += int64(r.Len())
    for col, arr := range rec.Columns() {
        size := arr.Data().SizeInBytes()
        pw.writtenUncompressed += size
        
        // 按列分组统计
        for _, columnGroup := range pw.columnGroups {
            if lo.Contains(columnGroup.Columns, col) {
                pw.columnGroupUncompressed[columnGroup.GroupID] += size
                break
            }
        }
    }
    
    // 3. 调用 C++ 底层写入
    defer rec.Release()
    return pw.writer.WriteRecordBatch(rec)
}
```

## 3. 数据读取流程

### 3.1 读取架构图

```
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   File Paths    │    │  packedRecord   │    │   Application   │
│   (文件列表)     │───▶│    Reader       │───▶│     Layer       │
└─────────────────┘    └─────────────────┘    └─────────────────┘
         │                        │                        │
         ▼                        ▼                        ▼
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│  Storage Files  │    │  Arrow Records  │    │   Query Result  │
│   (存储文件)     │    │   (批量数据)    │    │   (结果数据)    │
└─────────────────┘    └─────────────────┘    └─────────────────┘
```

### 3.2 PackedReader 创建

#### 3.2.1 Reader 初始化
```go
func NewPackedReader(
    filePaths []string,               // 文件路径列表
    schema *arrow.Schema,             // Arrow Schema
    bufferSize int64,                 // 读取缓冲区大小
    storageConfig *indexpb.StorageConfig, // 存储配置
) (*PackedReader, error) {
    
    // 1. 转换文件路径为 C 字符串
    cFilePaths := make([]*C.char, len(filePaths))
    for i, path := range filePaths {
        cFilePaths[i] = C.CString(path)
        defer C.free(unsafe.Pointer(cFilePaths[i]))
    }
    cFilePathsArray := (**C.char)(unsafe.Pointer(&cFilePaths[0]))
    cNumPaths := C.int64_t(len(filePaths))

    // 2. 导出 Arrow Schema
    var cas cdata.CArrowSchema
    cdata.ExportArrowSchema(schema, &cas)
    cSchema := (*C.struct_ArrowSchema)(unsafe.Pointer(&cas))
    defer cdata.ReleaseCArrowSchema(&cas)

    cBufferSize := C.int64_t(bufferSize)

    // 3. 创建存储配置
    var cPackedReader C.CPackedReader
    var status C.CStatus

    if storageConfig != nil {
        cStorageConfig := C.CStorageConfig{
            address:                C.CString(storageConfig.GetAddress()),
            bucket_name:            C.CString(storageConfig.GetBucketName()),
            access_key_id:          C.CString(storageConfig.GetAccessKeyID()),
            access_key_value:       C.CString(storageConfig.GetSecretAccessKey()),
            root_path:              C.CString(storageConfig.GetRootPath()),
            storage_type:           C.CString(storageConfig.GetStorageType()),
            // ... 其他配置项
        }
        // 内存清理 defer 语句...
        
        // 4. 调用 C API 创建 Reader
        status = C.NewPackedReaderWithStorageConfig(
            cFilePathsArray, cNumPaths, cSchema, 
            cBufferSize, cStorageConfig, &cPackedReader)
    } else {
        status = C.NewPackedReader(
            cFilePathsArray, cNumPaths, cSchema, 
            cBufferSize, &cPackedReader)
    }
    
    if err := ConsumeCStatusIntoError(&status); err != nil {
        return nil, err
    }
    
    return &PackedReader{
        cPackedReader: cPackedReader, 
        schema: schema,
    }, nil
}
```

### 3.3 数据批量读取

#### 3.3.1 ReadNext 实现
```go
func (pr *PackedReader) ReadNext() (arrow.Record, error) {
    // 1. 检查 Reader 状态
    if pr.cPackedReader == nil {
        return nil, io.EOF
    }

    // 2. 释放上一批数据
    if pr.currentBatch != nil {
        pr.currentBatch.Release()
        pr.currentBatch = nil
    }
    
    // 3. 从 C++ 层读取数据
    var cArr C.CArrowArray
    var cSchema C.CArrowSchema
    status := C.ReadNext(pr.cPackedReader, &cArr, &cSchema)
    if err := ConsumeCStatusIntoError(&status); err != nil {
        return nil, err
    }

    if cArr == nil {
        return nil, io.EOF // 数据流结束
    }

    // 4. 转换为 Go Arrow Record
    goCArr := (*cdata.CArrowArray)(unsafe.Pointer(cArr))
    goCSchema := (*cdata.CArrowSchema)(unsafe.Pointer(cSchema))
    defer func() {
        cdata.ReleaseCArrowArray(goCArr)
        cdata.ReleaseCArrowSchema(goCSchema)
    }()
    
    recordBatch, err := cdata.ImportCRecordBatch(goCArr, goCSchema)
    if err != nil {
        return nil, fmt.Errorf("failed to convert ArrowArray to Record: %w", err)
    }
    
    pr.currentBatch = recordBatch
    return recordBatch, nil
}
```

### 3.4 高级读取器实现

#### 3.4.1 packedRecordReader
```go
type packedRecordReader struct {
    paths         [][]string              // 分批文件路径
    chunk         int                     // 当前批次索引
    reader        *packed.PackedReader    // 底层读取器
    
    bufferSize    int64                   // 缓冲区大小
    arrowSchema   *arrow.Schema           // Arrow Schema
    field2Col     map[FieldID]int         // 字段到列映射
    storageConfig *indexpb.StorageConfig  // 存储配置
}
```

#### 3.4.2 批次迭代
```go
func (pr *packedRecordReader) iterateNextBatch() error {
    // 1. 关闭当前 Reader
    if pr.reader != nil {
        if err := pr.reader.Close(); err != nil {
            return err
        }
    }

    // 2. 检查是否还有批次
    if pr.chunk >= len(pr.paths) {
        return io.EOF
    }

    // 3. 创建新的 Reader
    reader, err := packed.NewPackedReader(
        pr.paths[pr.chunk], 
        pr.arrowSchema, 
        pr.bufferSize, 
        pr.storageConfig,
    )
    pr.chunk++
    if err != nil {
        return merr.WrapErrParameterInvalid(
            "New binlog record packed reader error: %s", err.Error())
    }
    
    pr.reader = reader
    return nil
}
```

#### 3.4.3 记录读取
```go
func (pr *packedRecordReader) Next() (Record, error) {
    if pr.reader == nil {
        if err := pr.iterateNextBatch(); err != nil {
            return nil, err
        }
    }

    for {
        rec, err := pr.reader.ReadNext()
        if err == io.EOF {
            // 当前批次结束，尝试下一批次
            if err := pr.iterateNextBatch(); err != nil {
                return nil, err
            }
            continue
        } else if err != nil {
            return nil, err
        }
        
        // 返回包装的 Record
        return NewSimpleArrowRecord(rec, pr.field2Col), nil
    }
}
```

## 4. 数据格式转换

### 4.1 Schema 转换

#### 4.1.1 Milvus Schema 到 Arrow Schema
```go
func ConvertToArrowSchema(schema *schemapb.CollectionSchema) (*arrow.Schema, error) {
    fields := make([]arrow.Field, 0, len(schema.Fields))
    
    for _, field := range schema.Fields {
        arrowField, err := convertFieldToArrow(field)
        if err != nil {
            return nil, err
        }
        fields = append(fields, arrowField)
    }
    
    return arrow.NewSchema(fields, nil), nil
}
```

#### 4.1.2 数据类型映射
```go
func convertFieldToArrow(field *schemapb.FieldSchema) (arrow.Field, error) {
    var dataType arrow.DataType
    
    switch field.DataType {
    case schemapb.DataType_Bool:
        dataType = arrow.FixedWidthTypes.Boolean
    case schemapb.DataType_Int8:
        dataType = arrow.PrimitiveTypes.Int8
    case schemapb.DataType_Int16:
        dataType = arrow.PrimitiveTypes.Int16
    case schemapb.DataType_Int32:
        dataType = arrow.PrimitiveTypes.Int32
    case schemapb.DataType_Int64:
        dataType = arrow.PrimitiveTypes.Int64
    case schemapb.DataType_Float:
        dataType = arrow.PrimitiveTypes.Float32
    case schemapb.DataType_Double:
        dataType = arrow.PrimitiveTypes.Double
    case schemapb.DataType_String, schemapb.DataType_VarChar:
        dataType = arrow.BinaryTypes.String
    case schemapb.DataType_FloatVector:
        dim := getDimensionFromTypeParams(field.TypeParams)
        dataType = arrow.ListOf(arrow.PrimitiveTypes.Float32)
    case schemapb.DataType_BinaryVector:
        dim := getDimensionFromTypeParams(field.TypeParams)
        dataType = arrow.BinaryTypes.Binary
    // ... 其他类型映射
    }
    
    return arrow.Field{
        Name: field.Name,
        Type: dataType,
        Nullable: !field.IsPrimaryKey,
    }, nil
}
```

### 4.2 数据构建

#### 4.2.1 Arrow Record 构建
```go
func BuildRecord(builder *array.RecordBuilder, 
                chunk *storage.InsertData, 
                schema *schemapb.CollectionSchema) error {
    
    for i, field := range schema.Fields {
        fieldBuilder := builder.Field(i)
        data := chunk.Data[field.FieldID]
        
        switch field.DataType {
        case schemapb.DataType_Int64:
            int64Builder := fieldBuilder.(*array.Int64Builder)
            int64Data := data.(*storage.Int64FieldData)
            for _, v := range int64Data.Data {
                int64Builder.Append(v)
            }
            
        case schemapb.DataType_FloatVector:
            listBuilder := fieldBuilder.(*array.ListBuilder)
            vectorData := data.(*storage.FloatVectorFieldData)
            dim := int(field.TypeParams[common.DimKey])
            
            for _, vector := range vectorData.Data {
                listBuilder.Append(true)
                valueBuilder := listBuilder.ValueBuilder().(*array.Float32Builder)
                for j := 0; j < dim; j++ {
                    valueBuilder.Append(vector[j])
                }
            }
            
        // ... 其他类型处理
        }
    }
    
    return nil
}
```

## 5. 性能优化机制

### 5.1 批处理优化

#### 5.1.1 缓冲区管理
- **可配置缓冲区大小**: 通过 `bufferSize` 参数调整内存使用
- **批量数据处理**: 一次处理多条记录减少系统调用
- **内存复用**: Arrow 内存池管理减少分配开销

#### 5.1.2 分片上传
- **多部分上传**: `multiPartUploadSize` 配置大文件分片
- **并行上传**: 多个文件片并行写入存储
- **断点续传**: 支持上传失败后的恢复

### 5.2 列式存储优化

#### 5.2.1 列分组策略
```go
func SplitBySchema(fields []*schemapb.FieldSchema) []ColumnGroup {
    // 默认每个字段一个分组，支持自定义分组策略
    var groups []ColumnGroup
    for i, field := range fields {
        group := ColumnGroup{
            GroupID: field.FieldID,
            Columns: []int{i},
        }
        groups = append(groups, group)
    }
    return groups
}
```

#### 5.2.2 零拷贝传输
- **C Data Interface**: 使用 Arrow C ABI 避免序列化
- **内存映射**: 大文件支持 mmap 访问
- **直接内存操作**: 减少数据拷贝开销

## 6. 错误处理和恢复

### 6.1 状态管理
```go
// C 状态码处理
func ConsumeCStatusIntoError(status *C.CStatus) error {
    if status.error_code == 0 {
        return nil
    }
    
    errorMsg := C.GoString(status.error_msg)
    C.free(unsafe.Pointer(status.error_msg))
    
    return errors.New(errorMsg)
}
```

### 6.2 资源清理
```go
func (pr *PackedReader) Close() error {
    if pr.cPackedReader == nil {
        return nil
    }
    
    // 1. 释放当前批次数据
    if pr.currentBatch != nil {
        pr.currentBatch.Release()
    }
    
    // 2. 关闭 C++ Reader
    status := C.CloseReader(pr.cPackedReader)
    if err := ConsumeCStatusIntoError(&status); err != nil {
        return err
    }
    
    pr.cPackedReader = nil
    return nil
}
```

StorageV2 的数据读写流程通过现代化的设计和实现，为 Milvus 提供了高性能、可扩展的数据访问能力，是整个向量数据库存储系统的核心基础。