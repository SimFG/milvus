# Milvus StorageV2 缓存机制与内存管理

## 1. 概述

StorageV2 的内存管理系统采用多层缓存策略，结合 Arrow 内存池和 MMAP 技术，实现了高效的数据缓存和内存管理。整个系统包括：Arrow 内存池管理、MMAP 文件缓存、数据块缓存等多个层次。

## 2. 内存管理架构

### 2.1 整体架构图

```
┌─────────────────────────────────────────────────────┐
│                  Application Layer                 │
│         (Query Processing, Index Building)         │
├─────────────────────────────────────────────────────┤
│                Arrow Memory Pool                   │
│  ┌─────────────────┐    ┌─────────────────────────┐  │
│  │ DefaultAllocator│    │   SystemAllocator      │  │
│  └─────────────────┘    └─────────────────────────┘  │
├─────────────────────────────────────────────────────┤
│                  MMAP Manager                      │
│  ┌─────────────────┐    ┌─────────────────────────┐  │
│  │ MmapChunkManager│    │   MmapBlocksHandler    │  │
│  └─────────────────┘    └─────────────────────────┘  │
├─────────────────────────────────────────────────────┤
│                   Block Cache                      │
│  ┌─────────────────┐    ┌─────────────────────────┐  │
│  │   Fixed Blocks  │    │   Variable Blocks      │  │
│  └─────────────────┘    └─────────────────────────┘  │
├─────────────────────────────────────────────────────┤
│                 Physical Storage                   │
│          (Memory Files, Disk Files)               │
└─────────────────────────────────────────────────────┘
```

### 2.2 核心组件关系

```go
// StorageV2 中的内存管理组件
type MemoryManagement struct {
    // Arrow 内存分配器
    ArrowAllocator   memory.Allocator
    
    // MMAP 管理器
    MmapManager      *MmapManager
    
    // 块缓存管理
    BlocksHandler    *MmapBlocksHandler
    
    // 缓存配置
    CacheConfig      CacheConfig
}
```

## 3. Arrow 内存池管理

### 3.1 内存分配器

#### 3.1.1 DefaultAllocator 使用
```go
import (
    "github.com/apache/arrow/go/v17/arrow/memory"
)

// 在 StorageV2 中广泛使用的默认分配器
func createArrowRecord(schema *arrow.Schema, data []interface{}) arrow.Record {
    // 使用默认内存分配器
    builder := array.NewRecordBuilder(memory.DefaultAllocator, schema)
    defer builder.Release()
    
    // 构建数据
    for i, field := range schema.Fields() {
        fieldBuilder := builder.Field(i)
        appendDataToBuilder(fieldBuilder, data[i], field.Type)
    }
    
    // 返回 Record（内存由 Arrow 池管理）
    return builder.NewRecord()
}
```

#### 3.1.2 内存池特性
- **自动内存管理**: Arrow 自动处理内存分配和释放
- **引用计数**: 使用智能指针管理内存生命周期
- **内存对齐**: 优化向量化操作的内存布局
- **批量分配**: 减少系统调用开销

### 3.2 内存使用模式

#### 3.2.1 Record 生命周期管理
```go
func processArrowRecord(rec arrow.Record) error {
    // Record 使用引用计数
    defer rec.Release() // 关键：释放引用
    
    // 处理每一列
    for i, col := range rec.Columns() {
        // 列数据也使用引用计数
        processColumn(col)
        // col 会在 rec.Release() 时自动释放
    }
    
    return nil
}

func copyRecordColumn(rec arrow.Record, colIdx int) arrow.Array {
    col := rec.Column(colIdx)
    
    // 增加引用计数，避免过早释放
    col.Retain()
    
    return col // 调用者负责释放
}
```

#### 3.2.2 Builder 模式优化
```go
func buildLargeBatch(schema *arrow.Schema, dataChunks [][]interface{}) arrow.Record {
    builder := array.NewRecordBuilder(memory.DefaultAllocator, schema)
    defer builder.Release()
    
    // 预分配容量，减少重分配
    for i, field := range schema.Fields() {
        fieldBuilder := builder.Field(i)
        reserveCapacity(fieldBuilder, calculateTotalRows(dataChunks))
    }
    
    // 批量填充数据
    for _, chunk := range dataChunks {
        for i, data := range chunk {
            appendToBuilder(builder.Field(i), data)
        }
    }
    
    return builder.NewRecord()
}

func reserveCapacity(builder array.Builder, capacity int) {
    switch b := builder.(type) {
    case *array.Int64Builder:
        b.Reserve(capacity)
    case *array.Float32Builder:
        b.Reserve(capacity)
    case *array.BinaryBuilder:
        b.Reserve(capacity)
    // ... 其他类型
    }
}
```

## 4. MMAP 内存管理

### 4.1 MmapManager 单例设计

#### 4.1.1 单例模式实现
```cpp
class MmapManager {
private:
    MmapManager() = default;

public:
    static MmapManager& GetInstance() {
        static MmapManager instance;
        return instance;
    }
    
    // 初始化配置
    void Init(const MmapConfig& config) {
        if (init_flag_ == false) {
            std::lock_guard<std::mutex> lock(init_mutex_);
            mmap_config_ = config;
            
            if (mcm_ == nullptr) {
                mcm_ = std::make_shared<MmapChunkManager>(
                    mmap_config_.mmap_path,
                    mmap_config_.disk_limit,
                    mmap_config_.fix_file_size);
            }
            
            init_flag_ = true;
        }
    }
    
    MmapChunkManagerPtr GetMmapChunkManager() {
        AssertInfo(init_flag_ == true, "Mmap manager has not been init.");
        return mcm_;
    }

private:
    mutable std::mutex init_mutex_;
    MmapConfig mmap_config_;
    MmapChunkManagerPtr mcm_ = nullptr;
    std::atomic<bool> init_flag_ = false;
};
```

#### 4.1.2 配置参数
```cpp
struct MmapConfig {
    std::string mmap_path;      // MMAP 文件路径
    uint64_t disk_limit;        // 磁盘限制
    uint64_t fix_file_size;     // 固定文件大小
    
    std::string ToString() const {
        return fmt::format("mmap_path: {}, disk_limit: {}, fix_file_size: {}", 
                          mmap_path, disk_limit, fix_file_size);
    }
};
```

### 4.2 MmapChunkManager 实现

#### 4.2.1 基本结构
```cpp
class MmapChunkManager {
public:
    explicit MmapChunkManager(std::string root_path,
                              const uint64_t disk_limit,
                              const uint64_t file_size);
    
    // 注册描述符
    MmapChunkDescriptorPtr Register();
    void UnRegister(const MmapChunkDescriptorPtr descriptor);
    
    // 内存分配
    void* Allocate(const MmapChunkDescriptorPtr descriptor, const uint64_t size);
    
    // 统计信息
    uint64_t GetDiskAllocSize();
    uint64_t GetDiskUsage();

private:
    mutable std::shared_mutex mtx_;
    
    // 块表：描述符ID -> 块列表
    std::unordered_map<MmapChunkDescriptor::ID, std::vector<MmapBlockPtr>> blocks_table_;
    
    // 块处理器
    std::unique_ptr<MmapBlocksHandler> blocks_handler_;
    
    std::string mmap_file_prefix_;
    std::atomic<uint64_t> descriptor_counter_;
};
```

#### 4.2.2 描述符管理
```cpp
class MmapChunkDescriptor {
public:
    using ID = uint64_t;
    
protected:
    friend class MmapChunkManager;
    const ID key_id_;
    
    explicit MmapChunkDescriptor(ID id) : key_id_(id) {}
    
public:
    ID GetId() const { return key_id_; }
};

// 注册新的描述符
MmapChunkDescriptorPtr MmapChunkManager::Register() {
    std::unique_lock<std::shared_mutex> lck(mtx_);
    
    auto id = descriptor_counter_.fetch_add(1);
    auto descriptor = std::shared_ptr<MmapChunkDescriptor>(
        new MmapChunkDescriptor(id));
    
    // 初始化块表项
    blocks_table_[id] = std::vector<MmapBlockPtr>();
    
    return descriptor;
}
```

### 4.3 内存块管理

#### 4.3.1 MmapBlock 设计
```cpp
struct MmapBlock {
public:
    enum class BlockType {
        Fixed = 0,      // 固定大小块
        Variable = 1,   // 可变大小块
    };
    
    MmapBlock(const std::string& file_name,
              const uint64_t file_size,
              BlockType type = BlockType::Fixed);
    
    void Init();        // 初始化内存映射
    void Close();       // 关闭映射
    void* Get(const uint64_t size);  // 分配内存
    void Reset() { offset_.store(0); }  // 重置偏移量
    
    uint64_t GetCapacity() { return file_size_; }
    
    // 全局统计
    static uint64_t TotalBlocksSize() { return allocated_size_.load(); }

private:
    const std::string file_name_;
    const uint64_t file_size_;
    char* addr_ = nullptr;              // 映射地址
    std::atomic<uint64_t> offset_ = 0;  // 当前偏移
    const BlockType block_type_;
    std::atomic<bool> is_valid_ = false;
    
    // 全局已分配大小
    static inline std::atomic<uint64_t> allocated_size_ = 0;
    mutable std::mutex file_mutex_;
};
```

#### 4.3.2 内存分配实现
```cpp
void* MmapBlock::Get(const uint64_t size) {
    if (!is_valid_.load()) {
        return nullptr;
    }
    
    std::lock_guard<std::mutex> lock(file_mutex_);
    
    uint64_t current_offset = offset_.load();
    if (current_offset + size > file_size_) {
        return nullptr;  // 空间不足
    }
    
    // 更新偏移量
    offset_.store(current_offset + size);
    
    return addr_ + current_offset;
}

void MmapBlock::Init() {
    // 创建临时文件
    int fd = open(file_name_.c_str(), O_CREAT | O_RDWR, 0644);
    if (fd == -1) {
        throw std::runtime_error("Failed to create mmap file");
    }
    
    // 扩展文件大小
    if (ftruncate(fd, file_size_) == -1) {
        close(fd);
        throw std::runtime_error("Failed to resize mmap file");
    }
    
    // 内存映射
    addr_ = (char*)mmap(nullptr, file_size_, 
                       PROT_READ | PROT_WRITE, 
                       MAP_SHARED, fd, 0);
    close(fd);
    
    if (addr_ == MAP_FAILED) {
        throw std::runtime_error("Failed to mmap file");
    }
    
    // 更新全局统计
    allocated_size_.fetch_add(file_size_);
    is_valid_.store(true);
}
```

### 4.4 块缓存管理

#### 4.4.1 MmapBlocksHandler 设计
```cpp
class MmapBlocksHandler {
public:
    MmapBlocksHandler(const uint64_t disk_limit,
                      const uint64_t fix_file_size,
                      const std::string file_prefix);
    
    // 分配固定大小块
    MmapBlockPtr AllocateFixSizeBlock();
    
    // 分配大块（可变大小）
    MmapBlockPtr AllocateLargeBlock(const uint64_t size);
    
    // 回收块
    void Deallocate(MmapBlockPtr&& block);
    
    // 容量管理
    uint64_t Capacity() { return MmapBlock::TotalBlocksSize(); }
    uint64_t Size();

private:
    uint64_t max_disk_limit_;
    std::string mmap_file_prefix_;
    std::atomic<uint64_t> mmmap_file_counter_;
    uint64_t fix_mmap_file_size_;
    
    // 固定大小块缓存队列
    std::queue<MmapBlockPtr> fix_size_blocks_cache_;
    const float cache_threshold = 0.25;  // 缓存阈值
    
    void FitCache(const uint64_t size);   // 缓存适配
    void ClearCache();                    // 清理缓存
};
```

#### 4.4.2 块分配策略
```cpp
MmapBlockPtr MmapBlocksHandler::AllocateFixSizeBlock() {
    // 1. 检查缓存
    if (!fix_size_blocks_cache_.empty()) {
        auto block = std::move(fix_size_blocks_cache_.front());
        fix_size_blocks_cache_.pop();
        block->Reset();  // 重置为可用状态
        return block;
    }
    
    // 2. 检查磁盘限制
    if (Capacity() + fix_mmap_file_size_ > max_disk_limit_) {
        FitCache(fix_mmap_file_size_);
    }
    
    // 3. 创建新块
    auto file_path = GetMmapFilePath();
    auto block = std::make_unique<MmapBlock>(
        file_path, fix_mmap_file_size_, MmapBlock::BlockType::Fixed);
    block->Init();
    
    return block;
}

MmapBlockPtr MmapBlocksHandler::AllocateLargeBlock(const uint64_t size) {
    // 大块不使用缓存，直接分配
    if (Capacity() + size > max_disk_limit_) {
        FitCache(size);
    }
    
    auto file_path = GetMmapFilePath();
    auto block = std::make_unique<MmapBlock>(
        file_path, size, MmapBlock::BlockType::Variable);
    block->Init();
    
    return block;
}
```

#### 4.4.3 缓存回收策略
```cpp
void MmapBlocksHandler::Deallocate(MmapBlockPtr&& block) {
    if (block->GetType() == MmapBlock::BlockType::Fixed) {
        // 固定大小块可以缓存复用
        if (fix_size_blocks_cache_.size() < 
            max_disk_limit_ * cache_threshold / fix_mmap_file_size_) {
            fix_size_blocks_cache_.push(std::move(block));
            return;
        }
    }
    
    // 不符合缓存条件，直接释放
    block.reset();
}

void MmapBlocksHandler::FitCache(const uint64_t size) {
    // 清理缓存以腾出空间
    while (!fix_size_blocks_cache_.empty() && 
           Capacity() + size > max_disk_limit_) {
        fix_size_blocks_cache_.pop();
    }
}
```

## 5. Go 语言内存管理集成

### 5.1 MMAP 接口封装

#### 5.1.1 Go 语言 MMAP 支持
```go
import "golang.org/x/exp/mmap"

func (lcm *LocalChunkManager) Mmap(ctx context.Context, filePath string) (*mmap.ReaderAt, error) {
    reader, err := mmap.Open(path.Clean(filePath))
    if errors.Is(err, os.ErrNotExist) {
        return nil, merr.WrapErrIoKeyNotFound(filePath, err.Error())
    }
    
    return reader, merr.WrapErrIoFailed(filePath, err)
}
```

#### 5.1.2 内存映射读取器
```go
type MmapReader struct {
    *mmap.ReaderAt
    size int64
}

func (mr *MmapReader) Read(p []byte) (n int, err error) {
    return mr.ReadAt(p, 0)
}

func (mr *MmapReader) Size() (int64, error) {
    return mr.size, nil
}

func (mr *MmapReader) Close() error {
    return mr.ReaderAt.Close()
}
```

### 5.2 缓存层实现

#### 5.2.1 LRU 缓存设计
```go
type LRUCache struct {
    capacity int
    cache    map[string]*list.Element
    list     *list.List
    mutex    sync.RWMutex
}

type CacheEntry struct {
    key   string
    value interface{}
    size  int64
}

func NewLRUCache(capacity int) *LRUCache {
    return &LRUCache{
        capacity: capacity,
        cache:    make(map[string]*list.Element),
        list:     list.New(),
    }
}

func (c *LRUCache) Get(key string) (interface{}, bool) {
    c.mutex.Lock()
    defer c.mutex.Unlock()
    
    if elem, ok := c.cache[key]; ok {
        // 移动到链表头部（最近使用）
        c.list.MoveToFront(elem)
        return elem.Value.(*CacheEntry).value, true
    }
    
    return nil, false
}

func (c *LRUCache) Put(key string, value interface{}, size int64) {
    c.mutex.Lock()
    defer c.mutex.Unlock()
    
    if elem, ok := c.cache[key]; ok {
        // 更新现有条目
        c.list.MoveToFront(elem)
        entry := elem.Value.(*CacheEntry)
        entry.value = value
        entry.size = size
        return
    }
    
    // 添加新条目
    entry := &CacheEntry{key: key, value: value, size: size}
    elem := c.list.PushFront(entry)
    c.cache[key] = elem
    
    // 检查容量限制
    c.evictIfNeeded()
}

func (c *LRUCache) evictIfNeeded() {
    for c.list.Len() > c.capacity {
        // 移除最少使用的条目
        elem := c.list.Back()
        if elem != nil {
            c.list.Remove(elem)
            entry := elem.Value.(*CacheEntry)
            delete(c.cache, entry.key)
        }
    }
}
```

#### 5.2.2 分层缓存策略
```go
type TieredCache struct {
    l1Cache    *LRUCache    // 内存缓存
    l2Cache    *DiskCache   // 磁盘缓存
    stats      CacheStats   // 缓存统计
}

func (tc *TieredCache) Get(key string) ([]byte, error) {
    // 1. 尝试 L1 缓存
    if data, ok := tc.l1Cache.Get(key); ok {
        tc.stats.L1Hits++
        return data.([]byte), nil
    }
    
    // 2. 尝试 L2 缓存
    if data, err := tc.l2Cache.Get(key); err == nil {
        tc.stats.L2Hits++
        // 提升到 L1 缓存
        tc.l1Cache.Put(key, data, int64(len(data)))
        return data, nil
    }
    
    tc.stats.Misses++
    return nil, ErrCacheMiss
}

func (tc *TieredCache) Put(key string, data []byte) error {
    // 同时写入两级缓存
    tc.l1Cache.Put(key, data, int64(len(data)))
    return tc.l2Cache.Put(key, data)
}
```

## 6. 性能优化策略

### 6.1 内存预分配

#### 6.1.1 Arrow Builder 预分配
```go
func optimizedBuildRecord(schema *arrow.Schema, estimatedRows int) arrow.Record {
    builder := array.NewRecordBuilder(memory.DefaultAllocator, schema)
    defer builder.Release()
    
    // 为每个字段预分配容量
    for i, field := range schema.Fields() {
        fieldBuilder := builder.Field(i)
        
        switch fieldBuilder := fieldBuilder.(type) {
        case *array.Int64Builder:
            fieldBuilder.Reserve(estimatedRows)
        case *array.Float32Builder:
            fieldBuilder.Reserve(estimatedRows)
        case *array.BinaryBuilder:
            fieldBuilder.Reserve(estimatedRows)
            // 预估字符串总长度
            fieldBuilder.ReserveData(estimatedRows * 100) 
        }
    }
    
    return builder.NewRecord()
}
```

#### 6.1.2 批量内存分配
```go
type MemoryBatch struct {
    blocks [][]byte
    index  int
    size   int
}

func NewMemoryBatch(blockSize, batchSize int) *MemoryBatch {
    blocks := make([][]byte, batchSize)
    for i := range blocks {
        blocks[i] = make([]byte, blockSize)
    }
    
    return &MemoryBatch{
        blocks: blocks,
        index:  0,
        size:   blockSize,
    }
}

func (mb *MemoryBatch) GetBlock() []byte {
    if mb.index >= len(mb.blocks) {
        // 扩展批次
        newBlocks := make([][]byte, len(mb.blocks))
        for i := range newBlocks {
            newBlocks[i] = make([]byte, mb.size)
        }
        mb.blocks = append(mb.blocks, newBlocks...)
    }
    
    block := mb.blocks[mb.index]
    mb.index++
    return block
}
```

### 6.2 内存回收优化

#### 6.2.1 对象池模式
```go
type RecordPool struct {
    pool sync.Pool
}

func NewRecordPool(schema *arrow.Schema) *RecordPool {
    return &RecordPool{
        pool: sync.Pool{
            New: func() interface{} {
                return array.NewRecordBuilder(memory.DefaultAllocator, schema)
            },
        },
    }
}

func (rp *RecordPool) GetBuilder() *array.RecordBuilder {
    return rp.pool.Get().(*array.RecordBuilder)
}

func (rp *RecordPool) PutBuilder(builder *array.RecordBuilder) {
    // 重置 builder 状态
    for i := 0; i < builder.Schema().NumFields(); i++ {
        builder.Field(i).NewArray().Release()
    }
    rp.pool.Put(builder)
}
```

#### 6.2.2 引用计数优化
```go
type ManagedRecord struct {
    record   arrow.Record
    refCount int32
    mutex    sync.Mutex
}

func NewManagedRecord(record arrow.Record) *ManagedRecord {
    record.Retain() // 增加引用计数
    return &ManagedRecord{
        record:   record,
        refCount: 1,
    }
}

func (mr *ManagedRecord) Retain() {
    mr.mutex.Lock()
    defer mr.mutex.Unlock()
    
    if mr.refCount > 0 {
        mr.refCount++
        mr.record.Retain()
    }
}

func (mr *ManagedRecord) Release() {
    mr.mutex.Lock()
    defer mr.mutex.Unlock()
    
    if mr.refCount > 0 {
        mr.refCount--
        mr.record.Release()
        
        if mr.refCount == 0 {
            mr.record = nil // 避免重复释放
        }
    }
}
```

## 7. 监控和诊断

### 7.1 内存使用监控

#### 7.1.1 内存统计
```go
type MemoryStats struct {
    TotalAllocated    int64    // 总分配内存
    CurrentUsed       int64    // 当前使用内存
    PeakUsed          int64    // 峰值使用内存
    CacheHitRate      float64  // 缓存命中率
    MmapFileCount     int64    // MMAP 文件数量
    MmapTotalSize     int64    // MMAP 总大小
}

func (ms *MemoryStats) Update(allocated, used int64) {
    atomic.AddInt64(&ms.TotalAllocated, allocated)
    atomic.StoreInt64(&ms.CurrentUsed, used)
    
    // 更新峰值
    for {
        peak := atomic.LoadInt64(&ms.PeakUsed)
        if used <= peak || atomic.CompareAndSwapInt64(&ms.PeakUsed, peak, used) {
            break
        }
    }
}
```

#### 7.1.2 实时监控
```go
func StartMemoryMonitor(ctx context.Context, interval time.Duration) {
    ticker := time.NewTicker(interval)
    defer ticker.Stop()
    
    for {
        select {
        case <-ctx.Done():
            return
        case <-ticker.C:
            collectMemoryMetrics()
        }
    }
}

func collectMemoryMetrics() {
    var m runtime.MemStats
    runtime.ReadMemStats(&m)
    
    // Go 运行时内存统计
    metrics.MemoryUsage.WithLabelValues("heap").Set(float64(m.HeapInuse))
    metrics.MemoryUsage.WithLabelValues("stack").Set(float64(m.StackInuse))
    metrics.MemoryUsage.WithLabelValues("sys").Set(float64(m.Sys))
    
    // MMAP 内存统计
    mmapManager := MmapManager::GetInstance()
    metrics.MemoryUsage.WithLabelValues("mmap").Set(float64(mmapManager.GetDiskUsage()))
    
    // Arrow 内存统计
    allocator := memory.DefaultAllocator
    metrics.MemoryUsage.WithLabelValues("arrow").Set(float64(allocator.AllocatedBytes()))
}
```

### 7.2 性能调优

#### 7.2.1 内存泄漏检测
```go
type LeakDetector struct {
    allocations map[uintptr]AllocationInfo
    mutex      sync.RWMutex
}

type AllocationInfo struct {
    Size      int64
    Timestamp time.Time
    Stack     []uintptr
}

func (ld *LeakDetector) TrackAllocation(ptr uintptr, size int64) {
    ld.mutex.Lock()
    defer ld.mutex.Unlock()
    
    stack := make([]uintptr, 10)
    n := runtime.Callers(2, stack)
    
    ld.allocations[ptr] = AllocationInfo{
        Size:      size,
        Timestamp: time.Now(),
        Stack:     stack[:n],
    }
}

func (ld *LeakDetector) CheckLeaks(maxAge time.Duration) []AllocationInfo {
    ld.mutex.RLock()
    defer ld.mutex.RUnlock()
    
    var leaks []AllocationInfo
    now := time.Now()
    
    for _, info := range ld.allocations {
        if now.Sub(info.Timestamp) > maxAge {
            leaks = append(leaks, info)
        }
    }
    
    return leaks
}
```

StorageV2 的缓存机制和内存管理通过多层次的优化策略，实现了高效的内存利用和数据访问，为向量数据库提供了强大的性能保障。